{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Introduction\n",
    "**Todo:** Paragraph(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MP3 & AAC Codecs\n",
    "**Todo:** A paragraph for history of MP3 and AAC codecs\n",
    "\n",
    "As can be seen in the flow chart below, MP3 and AAC codecs use some similar steps to generate compressed audio and similar steps to decode the compressed bitstream back to audio PCM signal. These similar steps are: Modified Discrete Cosine Transform (MDCT), Scaling, Quantization, and [Huffman Coding](https://en.wikipedia.org/wiki/Huffman_coding). Therefore, we will be focusing more on those three and specially MDCT as the last two are well known in almost every compression scheme including zip or gz. Among differences of MP3 and AAC codecs, it can be seen that MP3 codec is using [Polyphase Quadrature Filter (PQF)](https://en.wikipedia.org/wiki/Polyphase_quadrature_filter) before the MDCT, and [Alias Reduction](https://en.wikipedia.org/wiki/Aliasing) after the MDCT. Contrarily, AAC codec is using [Temporal Noise Shaping (TNS)](https://en.wikipedia.org/wiki/Noise_shaping) and [Perceptual \n",
    "Noise Substitution (PNS)](https://ccrma.stanford.edu/~kapilkm/422/Audio%20Compression%20using%20Entropy%20Coding%20and%20PNS-1.pdf).\n",
    "\n",
    "![MP3 and AAC Encoders and Decoders](images/mp3aac.svg)\n",
    "\n",
    "To analyze the outcome of these two codecs, two sample wave files are prepared. One is a music instrument and the other is a vocal speech. They will be encoded to a compressed file with similar bitrates (128k) and they will be decoded back (regenerated) to a wave file again. Afterwards we will try to identify the lost information using spectograms.\n",
    "\n",
    "To do the file conversion, we can use FFmpeg. FFmpeg is a cross-platform software containing many audio and video codecs that can be used for conversions which can be attained from [here](https://ffmpeg.org/). To convert the piano wave file to MP3 format, we can use the command below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ffmpeg -y -i audio/piano.wav -b:a 128k audio/piano.mp3                        2> /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, to convert the piano wave file to AAC format, we can use the command below. Notice that it is best to specify the audio codec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ffmpeg -y -i audio/piano.wav -b:a 128k audio/piano.aac        2> /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To convert compressed audio files back to wave formats, we can use the following commands below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ffmpeg -y -i audio/piano.mp3 audio/piano_regenerated_mp3.wav      2> /dev/null\n",
    "!ffmpeg -y -i audio/piano.aac audio/piano_regenerated_aac.wav      2> /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To analyze the regenerated wave files, we use the following three techniques:\n",
    "\n",
    "1. **Compression ratio** which is the most common metric of compression. It can be calculated using the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def calc_ratio(original_file, other_file):\n",
    "    original_file_size = os.path.getsize(original_file)\n",
    "    other_file_size = os.path.getsize(other_file)\n",
    "    return round(other_file_size*100/original_file_size, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Audio spectograms** using the `matplotlib` module. Spectograms can show us amplitude of different frequencies per every moment of a wave file. It is useful to see what frequencies are modified by the codec. It can be drawn using the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import soundfile as sf\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Audio\n",
    "from ipywidgets import widgets, Layout\n",
    "\n",
    "\n",
    "def wave_specgram(wav_file, titles, size=(4,6), duration=None):    \n",
    "    data, rate = sf.read(wav_file)\n",
    "    if duration:\n",
    "        data = data[:rate*duration]    \n",
    "    if len(titles) > 1: titles.insert(1,\"\")\n",
    "    \n",
    "    widget = widgets.Output(layout=Layout(width='100%'))\n",
    "    with widget:\n",
    "        plt.figure(figsize=size)\n",
    "        plt.title(\"\\n\".join(titles))\n",
    "        plt.xlabel('Time')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.specgram(data, Fs=rate)\n",
    "        plt.show()        \n",
    "        display(Audio(wav_file))\n",
    "        \n",
    "    return widget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Perceptual Evaluation of Audio Quality (PEAQ)** which is a standardized algorithm used for objective measurements of perceived audio quality, and is based on generally accepted psychoacoustic principles. The overall goal of this algorithm is to obtain a quality measure similar to a Subjective Difference Grade (SDG) acquired from listening tests with human participants. This output is called the [Objective Difference Grade (ODG)](https://en.wikipedia.org/wiki/Objective_difference_grade). The ODG ranges from 0 to −4 and is defined as follows: \n",
    "\n",
    "        0 = Imperceptible\n",
    "       -1 = Perceptible, but not annoying\n",
    "       -2 = Slightly annoying\n",
    "       -3 = Annoying\n",
    "       -4 = Very annoying\n",
    "\n",
    "    Sadly, a complete implementation of PEAQ couldn't be found on the internet, so Kabal's Matlab implementation of algorithm (provided in [Stephen Welch and Matthew Cohen's Github repository](https://github.com/stephencwelch/Perceptual-Coding-In-Python)) is used outside of this notebook and results are provided for you.\n",
    "    \n",
    "    When running the PEAQ algorithm, one important thing to remember is that the audio shouldn't be shifted. Some audio codecs, specifically AAC, apply some paddings in their calculations. For our case, I noticed that there is 1024 sample padding in the regenerated wave files from AAC codecs. So I use the following code to shift them back and run the PEAQ algorithm on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def shift_wave(src_wav_file, dst_wav_file, shift_amount):\n",
    "    data, rate = sf.read(src_wav_file)\n",
    "    data = np.roll(data, shift_amount)\n",
    "    sf.write(file=dst_wav_file, data=data, samplerate=rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the code below, raw piano wave file is compared to the regenerated wave file from MP3 and AAC compressions. Each one of them contain spectogram, the ratio, and the calculated ODG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8da497c450ac45bd9a5d9e702f254e54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Output(layout=Layout(width='100%')), Output(layout=Layout(width='100%')), Output(layout=Layout(…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "w1 = wave_specgram(\"audio/piano.wav\", \n",
    "                   [\"Piano WAV\", \"Ratio: 100%\", \"PEAQ ODG: 0.176\"], duration=2)\n",
    "\n",
    "ratio = calc_ratio('audio/piano.wav','audio/piano.mp3')\n",
    "w2 = wave_specgram(\"audio/piano_regenerated_mp3.wav\", \n",
    "                   [\"Piano MP3\", f\"Ratio: {ratio}%\", \"PEAQ ODG: 0.133\"], duration=2)\n",
    "\n",
    "ratio = calc_ratio('audio/piano.wav','audio/piano.aac')\n",
    "shift_wave(\"audio/piano_regenerated_aac.wav\", \"audio/piano_regenerated_aac_shifted.wav\", -1024)\n",
    "w3 = wave_specgram(\"audio/piano_regenerated_aac_shifted.wav\", \n",
    "                   [\"Piano AAC\", f\"Ratio: {ratio}%\", \"PEAQ ODG: -0.158\"], duration=2)\n",
    "\n",
    "widgets.HBox([w1, w2, w3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Todo:** A paragraph describing what can be seen in the piano spectogram.\n",
    "\n",
    "Now let's do the same for the speech sample and observe the changes after the MP3 and AAC compressions. First, we will compress and then decompress it using FFmpeg, then we will show the spectogram, compression ratio, and the calculated ODG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ffmpeg -y -i audio/voice.wav -b:a 128k audio/voice.mp3         2> /dev/null\n",
    "!ffmpeg -y -i audio/voice.wav -b:a 128k audio/voice.aac         2> /dev/null\n",
    "!ffmpeg -y -i audio/voice.mp3 audio/voice_regenerated_mp3.wav   2> /dev/null\n",
    "!ffmpeg -y -i audio/voice.aac audio/voice_regenerated_aac.wav   2> /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44aa205c4c2c43eba1666d2ce9cd0b98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Output(layout=Layout(width='100%')), Output(layout=Layout(width='100%')), Output(layout=Layout(…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "w1 = wave_specgram(\"audio/voice.wav\", \n",
    "                   [\"Voice WAV\", \"Ratio: 100%\",\"PEAQ ODG: 0.221\"])\n",
    "\n",
    "ratio = calc_ratio('audio/voice.wav','audio/voice.mp3')\n",
    "w2 = wave_specgram(\"audio/voice_regenerated_mp3.wav\", \n",
    "                   [\"Voice MP3\", f\"Ratio: {ratio}%\", \"PEAQ ODG: 0.034\"])\n",
    "\n",
    "ratio = calc_ratio('audio/voice.wav','audio/voice.aac')\n",
    "shift_wave(\"audio/voice_regenerated_aac.wav\", \"audio/voice_regenerated_aac_shifted.wav\", -1024)\n",
    "w3 = wave_specgram(\"audio/voice_regenerated_aac_shifted.wav\", \n",
    "                   [\"Voice AAC\", f\"Ratio: {ratio}%\", \"PEAQ ODG: -0.335\"])\n",
    "\n",
    "widgets.HBox([w1, w2, w3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Todo:** A paragraph describing what can be seen in the piano spectogram."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fourier Transform\n",
    "**Todo:** A paragraph as a reminder of Fourier series, Fourier transform, and discrete cosine transform: \n",
    "\n",
    "\\begin{aligned} s_{\\scriptscriptstyle N}(x)={\\frac {A_{0}}{2}}+\\sum _{n=1}^{N}A_{n}\\cdot \\cos \\left({\\tfrac {2\\pi }{P}}nx-\\varphi _{n}\\right).\\end{aligned}\n",
    "\n",
    "The animation below shows an example of fourier transformation:\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/2/2b/Fourier_series_and_transform.gif)\n",
    "\n",
    "The animation below shows the convergence of equation with increment of N value:\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/3/37/Example_of_Fourier_Convergence.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MDCT\n",
    "**Todo:** A paragraph about the Modified Discrete Cosine Transform (MDCT). \n",
    "\n",
    "\\begin{aligned} X_{k}=\\sum _{{n=0}}^{{2N-1}}x_{n}\\cos \\left[{\\frac  {\\pi }{N}}\\left(n+{\\frac  {1}{2}}+{\\frac  {N}{2}}\\right)\\left(k+{\\frac  {1}{2}}\\right)\\right].\\end{aligned}\n",
    "\n",
    "Wikipedia says: a transform based on the type-IV discrete cosine transform (DCT-IV), with the additional property of being lapped: it is designed to be performed on consecutive blocks of a larger dataset, where subsequent blocks are overlapped so that the last half of one block coincides with the first half of the next block. This overlapping, in addition to the energy-compaction qualities of the DCT, makes the MDCT especially attractive for signal compression applications, since it helps to avoid artifacts stemming from the block boundaries. As a result of these advantages, the MDCT is the most widely used lossy compression technique in audio data compression. It is employed in most modern audio coding standards, including MP3, Dolby Digital (AC-3), Vorbis (Ogg), Windows Media Audio (WMA), ATRAC, Cook, Advanced Audio Coding (AAC), High-Definition Coding (HDC), LDAC, Dolby AC-4, and MPEG-H 3D Audio, as well as speech coding standards such as AAC-LD (LD-MDCT), G.722.1, G.729.1, CELT, and Opus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_freq_spectogram(data, rate, duration, title, size=(4,6)):\n",
    "    data = np.swapaxes(data, 0,1)\n",
    "    data = data[:,:rate*duration//512]\n",
    "    data = np.float64(data)\n",
    "    eps = 1e-10\n",
    "    data = np.log(eps + data ** 2)\n",
    "    data = np.flipud(data)\n",
    "    \n",
    "    widget = widgets.Output(layout=Layout(width='100%'))\n",
    "    with widget:\n",
    "        plt.figure(figsize=size)\n",
    "        plt.title(title)\n",
    "        plt.xlabel('Time')\n",
    "        plt.ylabel('Frequency Index')\n",
    "        plt.imshow(data, aspect='auto', extent=[0,duration,0,512])\n",
    "        plt.show()\n",
    "        \n",
    "    return widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8173271966c45c7acb720b51f150830",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Output(layout=Layout(width='100%')), Output(layout=Layout(width='100%')), Output(layout=Layout(…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import scipy.fft\n",
    "import math\n",
    "import mdct\n",
    "\n",
    "w1 = wave_specgram(\"audio/piano.wav\", [\"Piano WAV\"], duration=2)\n",
    "\n",
    "data, rate = sf.read(\"audio/piano.wav\")\n",
    "data.resize(math.ceil(len(data)/512)*512, refcheck=False)\n",
    "data = np.reshape(data, (-1, 512))\n",
    "data = scipy.fft.dct(data)\n",
    "w2 = draw_freq_spectogram(data, rate, 2, 'Piano DCT applied')\n",
    "\n",
    "data, rate = sf.read(\"audio/piano.wav\")\n",
    "data = mdct.mdct(data)\n",
    "data = data.swapaxes(0,1)\n",
    "w3 = draw_freq_spectogram(data, rate, 2, 'Piano MDCT applied')\n",
    "\n",
    "display(widgets.HBox([w1,w2, w3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Todo:** A paragraph about practically trying MDCT using artificial signals, starting with a constant signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def generate_freq_const(rate, duration):\n",
    "    data = np.zeros((rate*duration//512, 512),dtype=np.float64)    \n",
    "    data[:,30] = 5\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Todo:** Short description about a sine signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_freq_sine(rate, duration):\n",
    "    data = np.zeros((rate*duration//512, 512),dtype=np.float64)\n",
    "    for i in range(data.shape[0]):\n",
    "        data[i, int(15 + 10*np.sin(np.deg2rad(10*i))),] = 5\n",
    "    return data\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Todo:** Short description about drawing the MDCT spectogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's put it together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f06c68b764b4e759ce2c5a6a79d1a36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Output(layout=Layout(width='100%')), Output(layout=Layout(width='100%')), Output(layout=Layout(…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import scipy.fft\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "data = generate_freq_const(48000, 2)\n",
    "\n",
    "w1 = draw_freq_spectogram(data, 48000, 2, \"Spectogram Not-Inverted\")\n",
    "\n",
    "sig1 = scipy.fft.idct(data)\n",
    "sig1 = sig1.flatten()\n",
    "sf.write(\"audio/dct_const.wav\", sig1, 48000)\n",
    "w2 = wave_specgram(\"audio/dct_const.wav\", ['Spectogram Inversed with DCT'])\n",
    "\n",
    "sig2 = np.swapaxes(data, 0,1)\n",
    "sig2 = mdct.imdct(sig2)\n",
    "sf.write(\"audio/mdct_const.wav\", sig2, 48000)\n",
    "w3 = wave_specgram(\"audio/mdct_const.wav\", ['Spectogram Inversed with MDCT'])\n",
    "\n",
    "widgets.HBox([w1, w2, w3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Todo:** A paragraph describing what is observabale in the spectogram above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3612f52a5bb44881896f09e7b9aa6d06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Output(layout=Layout(width='100%')), Output(layout=Layout(width='100%')), Output(layout=Layout(…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = generate_freq_sine(48000, 2)\n",
    "\n",
    "w1 = draw_freq_spectogram(data, 48000, 2, \"Spectogram Not-Inverted\")\n",
    "\n",
    "sig1 = scipy.fft.idct(data)\n",
    "sig1 = sig1.flatten()\n",
    "sf.write(\"audio/mdct_const.wav\", sig1, 48000)\n",
    "w2 = wave_specgram(\"audio/mdct_const.wav\", ['Spectogram Inversed with DCT'])\n",
    "\n",
    "sig2 = np.swapaxes(data, 0,1)\n",
    "sig2 = mdct.imdct(sig2)\n",
    "sf.write(\"audio/mdct_sine.wav\", sig2, 48000)\n",
    "w3 = wave_specgram(\"audio/mdct_sine.wav\", ['Spectogram Inversed with MDCT'])\n",
    "\n",
    "widgets.HBox([w1, w2, w3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Todo:** A paragraph describing what is observabale in the spectogram above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder\n",
    "**Big To do:** Paragraphs describing how to make a compression. Need to describe each step: 1) High Freq Cut, 2) Float16, 3) Rounding Floats, 4) Removing Low Intensities, 5) using numpy's savez_compressed which already has scaling and huffman coding inside it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import soundfile as sf\n",
    "\n",
    "encoding_stages = []\n",
    "\n",
    "def encode(input_wav_file, output_npz_name):\n",
    "\n",
    "    steps_count = 5\n",
    "    global encoding_stages\n",
    "    encoding_stages.clear()\n",
    "    \n",
    "    def encode_write(data, step_number, step_name):        \n",
    "        encoding_stages.append(step_name)\n",
    "        output_file = f'{output_npz_name}{step_number}.npz'    \n",
    "        np.savez_compressed(output_file, rate=rate, data=data)        \n",
    "        ratio = calc_ratio(input_wav_file,output_file)\n",
    "        data = np.swapaxes(data, 0,1)\n",
    "        return draw_freq_spectogram(data, rate, 2, f'{step_name}\\n\\nRatio: {ratio}%', size=(2,6))\n",
    "    \n",
    "    data, rate = sf.read(input_wav_file)    \n",
    "    \n",
    "    r = mdct.mdct(data)\n",
    "    w1 = encode_write(r, step_number=0, step_name='Float64')    \n",
    "    \n",
    "    r = np.float16(r)\n",
    "    w2 = encode_write(r, step_number=1, step_name='Float16')\n",
    "\n",
    "    r[256:, :] = 0    \n",
    "    w3 = encode_write(r, step_number=2, step_name='High Freq Cut')    \n",
    "\n",
    "    r = np.round(r, decimals=3)\n",
    "    w4 = encode_write(r, step_number=3, step_name='Round Floats')   \n",
    "\n",
    "    r = np.where(abs(r) < 0.01, 0, r)\n",
    "    w5 = encode_write(r, step_number=4, step_name='Remove Low Intensities')    \n",
    "    \n",
    "    return widgets.HBox([w1,w2,w3,w4,w5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a1a7c0e747c438395c568050e0f6ce0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Output(layout=Layout(width='100%')), Output(layout=Layout(width='100%')), Output(layout=Layout(…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "encode('audio/piano.wav', 'audio/piano_npz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Todo:** Describe what is observable in the spectograms above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86eb58bb6cd14faf8d1ee17e8b29138c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Output(layout=Layout(width='100%')), Output(layout=Layout(width='100%')), Output(layout=Layout(…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "encode('audio/voice.wav', 'audio/voice_npz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Todo:** Describe what is observable in the spectograms above.\n",
    "\n",
    "# Decoder\n",
    "\n",
    "**Todo:** A paragraph about the decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(input_npz_file, output_wav_file):\n",
    "    npz = np.load(input_npz_file)\n",
    "    rate = npz['rate']\n",
    "    sig2 = mdct.imdct(npz['data'])\n",
    "    sf.write(output_wav_file, sig2, rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13ae43e77e7344b0bd8d22853bbc9e08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Output(layout=Layout(width='100%')), Output(layout=Layout(width='100%')), Output(layout=Layout(…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "w_list = []\n",
    "\n",
    "odg = [0.176, 0.122, 0.067, -1.097, -2.138]\n",
    "\n",
    "for i in range(len(encoding_stages)):    \n",
    "    decode(f'audio/piano_npz{i}.npz', f'audio/piano_regenerated_npz{i}.wav')\n",
    "    ratio = calc_ratio('audio/piano.wav',f'audio/piano_npz{i}.npz')\n",
    "    widget = wave_specgram(f'audio/piano_regenerated_npz{i}.wav', \n",
    "                           [encoding_stages[i], f\"Ratio: {ratio}%\", f\"PEAQ ODG: {odg[i]}\"], duration=2, size=(2,6))\n",
    "    w_list.append(widget)\n",
    "    \n",
    "widgets.HBox(w_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5618033755b4cdb866a5e1d844d6b47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Output(layout=Layout(width='100%')), Output(layout=Layout(width='100%')), Output(layout=Layout(…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "w_list = []\n",
    "\n",
    "odg = [0.211, 0.197, -0.270, -0.462, -2.975]\n",
    "\n",
    "for i in range(len(encoding_stages)):    \n",
    "    decode(f'audio/voice_npz{i}.npz', f'audio/voice_regenerated_npz{i}.wav')\n",
    "    ratio = calc_ratio('audio/voice.wav',f'audio/voice_npz{i}.npz')\n",
    "    widget = wave_specgram(f'audio/voice_regenerated_npz{i}.wav', \n",
    "                           [encoding_stages[i],f\"Ratio: {ratio}%\", f\"PEAQ ODG: {odg[i]}\"], size=(2,6))\n",
    "    w_list.append(widget)\n",
    "    \n",
    "widgets.HBox(w_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Todo:** Describe what is observable in the spectograms above.\n",
    "# Conclusion\n",
    "**Todo:** Some paragraphs here.\n",
    "\n",
    "# References (10-20)\n",
    "\n",
    "Ravelli, Emmanuel, Gal Richard, and Laurent Daudet. “Union of MDCT Bases for Audio Coding.” IEEE Transactions on Audio, Speech, and Language Processing 16, no. 8 (November 2008): 1361–72. https://doi.org/10.1109/TASL.2008.2004290.\n",
    "\n",
    "Ravelli, Emmanuel, Gaël Richard, and Laurent Daudet. “Audio Signal Representations for Indexing in the Transform Domain.” IEEE Transactions on Audio, Speech, and Language Processing 18, no. 3 (March 2010): 434–46. https://doi.org/10.1109/TASL.2009.2025099.\n",
    "\n",
    "P. Kabal, \"An Examination and Interpretation of ITU-R BS.1387: Perceptual Evaluation of Audio Quality\", TSP Lab Technical Report, Dept. Electrical & Computer Engineering, McGill University, May 2002. \n",
    "\n",
    "\n",
    "**To Read:**\n",
    "\n",
    "Stephen. Perceptual Coding In Python. Matlab, 2021. https://github.com/stephencwelch/Perceptual-Coding-In-Python/blob/2993f57570663768c02745019185091a23f021fe/PEAQ.md.\n",
    "\n",
    "“MUSHRA.” In Wikipedia, March 26, 2021. https://en.wikipedia.org/w/index.php?title=MUSHRA&oldid=1014261385.\n",
    "\n",
    "“Karlheinz Brandenburg.” Accessed November 18, 2021. https://scholar.google.de/citations?user=a9nMg8sAAAAJ&hl=de.\n",
    "\n",
    "To read what is coming up and what needs are next steps in audio compression schemes.\n",
    "\n",
    "https://github.com/gtzan/mir_program_kadenze/blob/master/course1/session3/kadenze_mir_c1_s3_4_monophonic_pitch_estimation.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
